{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# House Prices basic Feature Engineering baselining\n",
    "\n",
    "This notebook will illustrate how to build a basic pipeline for a regression problem. The same structure could be applied to a classification problem, of course. In this case, we will start from the cleaned dataset of the Kaggle competition on predicting House prices (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). The cleaning has been done from Dataiku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataiku Data Preparation Pipeline](./data/dataiku_dataprep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#House-Prices-basic-Feature-Engineering-baselining\" data-toc-modified-id=\"House-Prices-basic-Feature-Engineering-baselining-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>House Prices basic Feature Engineering baselining</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pipeline-design\" data-toc-modified-id=\"Pipeline-design-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Pipeline design</a></span></li><li><span><a href=\"#Read-and-Prepare-data\" data-toc-modified-id=\"Read-and-Prepare-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Read and Prepare data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Impute-with-the-default-value,-not-NA\" data-toc-modified-id=\"Impute-with-the-default-value,-not-NA-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Impute with the default value, not NA</a></span></li><li><span><a href=\"#Fix-type-of-feature\" data-toc-modified-id=\"Fix-type-of-feature-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Fix type of feature</a></span></li><li><span><a href=\"#Encoding-categorical-variables\" data-toc-modified-id=\"Encoding-categorical-variables-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Encoding categorical variables</a></span></li><li><span><a href=\"#Fix-Skewness-(we're-using-linear-regression)\" data-toc-modified-id=\"Fix-Skewness-(we're-using-linear-regression)-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Fix Skewness (we're using linear regression)</a></span></li></ul></li><li><span><a href=\"#Read-&amp;-Prepare-Data-Pipeline\" data-toc-modified-id=\"Read-&amp;-Prepare-Data-Pipeline-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Read &amp; Prepare Data Pipeline</a></span></li><li><span><a href=\"#Linear-Regression-Model\" data-toc-modified-id=\"Linear-Regression-Model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Linear Regression Model</a></span></li><li><span><a href=\"#Feature-Engineering-Pipeline\" data-toc-modified-id=\"Feature-Engineering-Pipeline-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Feature Engineering Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#SumSF,-sumBaths,-sumPorch\" data-toc-modified-id=\"SumSF,-sumBaths,-sumPorch-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>SumSF, sumBaths, sumPorch</a></span></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Outliers</a></span></li><li><span><a href=\"#Under-representation\" data-toc-modified-id=\"Under-representation-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Under-representation</a></span></li><li><span><a href=\"#Cross-Validation-evaluation\" data-toc-modified-id=\"Cross-Validation-evaluation-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Cross Validation evaluation</a></span></li></ul></li><li><span><a href=\"#Run-it!\" data-toc-modified-id=\"Run-it!-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Run it!</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Pipeline design\n",
    "\n",
    "Our basic pipeline will look like the function below. It is used with two arguments: the dataset and set of functions to try over the data. Internally, the function will loop over the specified function names to check if they improve the score (R2) of the model validated over the RAW test set. If so, the function is kept, and the data is updated according to that function. Otherwise, the function is simply rejected and the data suffer no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renero/Code/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer, RobustScaler\n",
    "\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(raw_data, fe_functions):\n",
    "    selected_functions = []\n",
    "    base_score = score_model(raw_data)\n",
    "    print('Base Score: {:.4f}'.format(base_score))\n",
    "    engineered_data = raw_data.copy()\n",
    "    for fe_function in fe_functions:\n",
    "        processed_data = globals()[fe_function](engineered_data)\n",
    "        new_score = score_model(processed_data)\n",
    "        print('- New Score ({}): {:.4f} '.format(fe_function, new_score), \n",
    "              end='')\n",
    "        difference = (new_score-base_score)\n",
    "        print('[diff: {:.4f}] '.format(difference), end='')\n",
    "        if difference > -0.01:\n",
    "            selected_functions.append(fe_function)\n",
    "            engineered_data = processed_data.copy()\n",
    "            base_score = new_score\n",
    "            print('[Accepted]')\n",
    "        else:\n",
    "            print('[Rejected]')\n",
    "    return selected_functions, engineered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work on functions to:\n",
    "  - read the data (`read_data`)\n",
    "  - run the model over a dataset (`score_model`)\n",
    "  - apply the different ideas that we come up with during the process\n",
    "  \n",
    "Notice that we haven't applied the split between training and test. That step will be done within the score_model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path):\n",
    "    raw_data = pd.read_csv(input_path, keep_default_na=False, na_values=['_'])\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute with the default value, not NA\n",
    "We documentation says that some of the features should take default values, and some others should be grouped as they're not very representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(df, column, old_value, new_value):\n",
    "    df.loc[df[column] == old_value, column] = new_value\n",
    "    \n",
    "def impute(df):\n",
    "    replace(df, 'Electrical', 'NA', 'SBrkr')\n",
    "    # Minority categories grouped into 'Other'\n",
    "    [replace(df, 'Exterior1st', minor, 'Other') for minor in \n",
    "     ['Stone', 'AsphShn', 'CBlock', 'ImStucc']]\n",
    "    [replace(df, 'Exterior2nd', minor, 'Other') for minor in \n",
    "     ['Stone', 'AsphShn', 'CBlock', 'ImStucc']]\n",
    "    [replace(df, 'SaleType', minor, 'Oth') for minor in \n",
    "     ['ConLD', 'ConLI', 'ConLw', 'Con']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix type of feature\n",
    "Change the type of some of the columns that look like numbers but they're really categories, to that: categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_types(df):\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Encoding categorical variables\n",
    "\n",
    "Need to encode categorical variables using one-hot encoding. I will detect which columns are not numerical, and will run one-hot encoding on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_features(df):\n",
    "    columns = df.columns\n",
    "    return df._get_numeric_data().columns\n",
    "\n",
    "def categorical_features(df):\n",
    "    numerical_columns = numerical_features(df)\n",
    "    return(list(set(df.columns) - set(numerical_columns)))\n",
    "\n",
    "def onehot_encode(df):\n",
    "    numericals = df.get(numerical_features(df))\n",
    "    new_df = numericals.copy()\n",
    "    for categorical_column in categorical_features(df):\n",
    "        new_df = pd.concat([new_df, \n",
    "                            pd.get_dummies(df[categorical_column], \n",
    "                                           prefix=categorical_column)], \n",
    "                           axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Skewness (we're using linear regression)\n",
    "\n",
    "Try to identify what features present high skewness to fix it with the BoxCox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_skewness(df):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', \n",
    "                      'float16', 'float32', 'float64']\n",
    "    numeric_features = []\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype in numeric_dtypes: \n",
    "            numeric_features.append(i)\n",
    "\n",
    "    feature_skew = df[numeric_features].apply(\n",
    "        lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skews = pd.DataFrame({'skew':feature_skew})\n",
    "    return feature_skew, numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know how to detect them, write a function to do all the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_skewness(df):\n",
    "    feature_skew, numeric_features = feature_skewness(df)\n",
    "    high_skew = feature_skew[feature_skew > 0.5]\n",
    "    skew_index = high_skew.index\n",
    "    \n",
    "    for i in skew_index:\n",
    "        df[i] = boxcox1p(df[i], boxcox_normmax(df[i]+1))\n",
    "\n",
    "    skew_features = df[numeric_features].apply(\n",
    "        lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skews = pd.DataFrame({'skew':skew_features})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Prepare Data Pipeline\n",
    "Transform RAW by imputing missing or rare values, fixing skewness and fixing some types that should not be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape before onehot encoding: (1460, 79)\n",
      "Dataset shape AFTER onehot encoding: (1460, 306)\n"
     ]
    }
   ],
   "source": [
    "raw = read_data('./data/houseprices_prepared.csv.gz').drop(['Id'], axis=1)\n",
    "prepared = fix_skewness(impute(fix_types(raw)))\n",
    "print('Original shape before onehot encoding: {}'.format(prepared.shape))\n",
    "dataset = pd.get_dummies(prepared)\n",
    "print('Dataset shape AFTER onehot encoding: {}'.format(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Linear Regression Model\n",
    "\n",
    "This function will be called to run a linear regression model over the dataset passed as argument. It splits it into train and test, trains and evaluate the test set to finally return the R2 metric as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(data):\n",
    "    X = data.loc[:, data.columns != 'SalePrice']\n",
    "    y = data.loc[:, 'SalePrice']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    # print('Coefficients: \\n', regr.coef_)\n",
    "    \n",
    "    # The metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Pipeline\n",
    "\n",
    "### SumSF, sumBaths, sumPorch\n",
    "\n",
    "Simply sum the square feet of the different parts of the house into a single column called HouseSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_SF(df):\n",
    "    columns_to_add = ['1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2']\n",
    "    df['House_SF'] = df[columns_to_add].fillna('').sum(axis=1)\n",
    "    df.drop(columns_to_add, axis=1)\n",
    "    return df\n",
    "\n",
    "def sum_Baths(df):\n",
    "    df['Total_Baths'] = (df['FullBath'] + \n",
    "                         df['BsmtFullBath'] + \n",
    "                         (0.5*df['HalfBath']) + \n",
    "                         (0.5*df['BsmtHalfBath']))\n",
    "    df.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1)\n",
    "    return df\n",
    "\n",
    "def sum_Porch(df):\n",
    "    columns_to_add = ['OpenPorchSF','3SsnPorch','EnclosedPorch',\n",
    "                      'ScreenPorch','WoodDeckSF']\n",
    "    df['Porch_sf'] = df[columns_to_add].sum(axis=1)\n",
    "    df.drop(columns_to_add, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Outliers\n",
    "We use the statsmodels package to solve this issue. Let's do a little bit more in-depth and rigorous analysis first on outliers. I'll employ Leave-One-Out methodology with OLS to find which points have a significant effect on our model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    ols = sm.OLS(endog = y, exog = X)\n",
    "    fit = ols.fit()\n",
    "    test = fit.outlier_test()['bonf(p)']\n",
    "    outliers = list(test[test<1e-3].index) \n",
    "    df.drop(df.index[outliers])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-representation\n",
    "\n",
    "Eliminate those columns with most of the information belonging to the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_represented_features(df):\n",
    "    under_rep = []\n",
    "    for i in df.columns:\n",
    "        counts = df[i].value_counts()\n",
    "        zeros = counts.iloc[0]\n",
    "        if ((zeros / len(df)) * 100) > 99.0:\n",
    "            under_rep.append(i)\n",
    "    df.drop(under_rep, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation evaluation\n",
    "\n",
    "Cross validation implies that instead of evaluating the model with a single split of train/test, we will use a k-fold technique, and that will produce different results. A realistic outcome would be the mean of all that process, but if you can check how far your model can reach by simply changing the training/test datasets using CV, I decided to take the 'max()' in the last line of the function. Try with the mean() to check some other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_evaluate(df):\n",
    "    lm = LinearRegression()\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=23)\n",
    "\n",
    "    def cv_r2(model):\n",
    "        r2 = cross_val_score(model, X, y, scoring='r2', cv=kfolds)\n",
    "        return r2\n",
    "    \n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    benchmark_model = make_pipeline(RobustScaler(), lm).fit(X=X, y=y)\n",
    "    return cv_r2(benchmark_model).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renero/Code/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score: 0.8766\n",
      "- New Score (sum_SF): 0.8766 [diff: 0.0000] [Accepted]\n",
      "- New Score (sum_Baths): 0.8766 [diff: -0.0000] [Accepted]\n",
      "- New Score (sum_Porch): 0.8766 [diff: 0.0000] [Accepted]\n",
      "- New Score (under_represented_features): 0.8771 [diff: 0.0006] [Accepted]\n",
      "Max R2 after CV: 0.9344\n"
     ]
    }
   ],
   "source": [
    "# I'm not adding the 'remove_outliers' to save time, since that step takes \n",
    "# some minutes to complete.\n",
    "# You should add it to check the results for yourself\n",
    "fe_functions = ['sum_SF','sum_Baths','sum_Porch','under_represented_features']\n",
    "funcs, new_dataset = feature_engineering_pipeline(dataset, fe_functions)\n",
    "\n",
    "# After running the FE pipeline, I check how CV make my results shuffle, \n",
    "# as it takes different portions of the data each time, and that causes \n",
    "# different metric values\n",
    "r2 = cv_evaluate(new_dataset)\n",
    "print('Max R2 after CV: {:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "309px",
    "width": "377px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
