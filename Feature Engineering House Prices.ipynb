{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "# House Prices basic Feature Engineering baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "This notebook will illustrate how to build a basic pipeline for a regression problem. The same structure could be applied to a classification problem, of course. In this case, we will start from the cleaned dataset of the Kaggle competition on predicting House prices (https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). The cleaning has been done from Dataiku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataiku Data Preparation Pipeline](./dataiku_dataprep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline design\n",
    "\n",
    "Our basic pipeline will look like the function below. It is used with two arguments: the dataset and set of functions to try over the data. Internally, the function will loop over the specified function names to check if they improve the score (R2) of the model validated over the RAW test set. If so, the function is kept, and the data is updated according to that function. Otherwise, the function is simply rejected and the data suffer no change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pipeline(raw_data, fe_functions):\n",
    "    selected_functions = []\n",
    "    base_score = score_model(raw_data)\n",
    "    print('Base Score: {:.4f}'.format(base_score))\n",
    "    engineered_data = raw_data.copy()\n",
    "    for fe_function in fe_functions:\n",
    "        processed_data = globals()[fe_function](engineered_data)\n",
    "        new_score = score_model(processed_data)\n",
    "        print('- New Score ({}): {:.4f} '.format(fe_function, new_score), end='')\n",
    "        difference = (new_score-base_score)\n",
    "        print('[diff: {:.4f}] '.format(difference), end='')\n",
    "        if difference > -0.01:\n",
    "            selected_functions.append(fe_function)\n",
    "            engineered_data = processed_data.copy()\n",
    "            base_score = new_score\n",
    "            print('[Accepted]')\n",
    "        else:\n",
    "            print('[Rejected]')\n",
    "    return selected_functions, engineered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to work on functions to:\n",
    "  - read the data (`read_data`)\n",
    "  - run the model over a dataset (`score_model`)\n",
    "  - apply the different ideas that we come up with during the process\n",
    "  \n",
    "Notice that we haven't applied the split between training and test. That step will be done within the score_model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(input_path):\n",
    "    raw_data = pd.read_csv(input_path, keep_default_na=False, na_values=['_'])\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute with the default value, not NA\n",
    "We documentation says that some of the features should take default values, and some others should be grouped as they're not very representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(df, column, old_value, new_value):\n",
    "    df.loc[df[column] == old_value, column] = new_value\n",
    "    \n",
    "def impute(df):\n",
    "    replace(df, 'Electrical', 'NA', 'SBrkr')\n",
    "    # Minority categories grouped into 'Other'\n",
    "    [replace(df, 'Exterior1st', minor, 'Other') for minor in ['Stone', 'AsphShn', 'CBlock', 'ImStucc']]\n",
    "    [replace(df, 'Exterior2nd', minor, 'Other') for minor in ['Stone', 'AsphShn', 'CBlock', 'ImStucc']]\n",
    "    [replace(df, 'SaleType', minor, 'Oth') for minor in ['ConLD', 'ConLI', 'ConLw', 'Con']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix type of feature\n",
    "Change the type of some of the columns that look like numbers but they're really categories, to that: categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_types(df):\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n",
    "\n",
    "Need to encode categorical variables using one-hot encoding. I will detect which columns are not numerical, and will run one-hot encoding on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def numerical_features(df):\n",
    "    columns = df.columns\n",
    "    return df._get_numeric_data().columns\n",
    "\n",
    "def categorical_features(df):\n",
    "    numerical_columns = numerical_features(df)\n",
    "    return(list(set(df.columns) - set(numerical_columns)))\n",
    "\n",
    "def onehot_encode(df):\n",
    "    numericals = df.get(numerical_features(df))\n",
    "    new_df = numericals.copy()\n",
    "    for categorical_column in categorical_features(df):\n",
    "        new_df = pd.concat([new_df, pd.get_dummies(df[categorical_column], prefix=categorical_column)], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Skewness (we're using linear regression)\n",
    "\n",
    "Try to identify what features present high skewness to fix it with the BoxCox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "def feature_skewness(df):\n",
    "    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = []\n",
    "    for i in df.columns:\n",
    "        if df[i].dtype in numeric_dtypes: \n",
    "            numeric_features.append(i)\n",
    "\n",
    "    feature_skew = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skews = pd.DataFrame({'skew':feature_skew})\n",
    "    return feature_skew, numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we know how to detect them, write a function to do all the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def fix_skewness(df):\n",
    "    feature_skew, numeric_features = feature_skewness(df)\n",
    "    high_skew = feature_skew[feature_skew > 0.5]\n",
    "    skew_index = high_skew.index\n",
    "    \n",
    "    for i in skew_index:\n",
    "        df[i] = boxcox1p(df[i], boxcox_normmax(df[i]+1))\n",
    "\n",
    "    skew_features = df[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    skews = pd.DataFrame({'skew':skew_features})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Prepare Data Pipeline\n",
    "Transform RAW by imputing missing or rare values, fixing skewness and fixing some types that should not be numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape before onehot encoding: (1460, 79)\n",
      "Dataset shape AFTER onehot encoding: (1460, 306)\n"
     ]
    }
   ],
   "source": [
    "raw = read_data('./all/train_complete_prepared.csv').drop(['Id'], axis=1)\n",
    "prepared = fix_skewness(impute(fix_types(raw)))\n",
    "print('Original dataset shape before onehot encoding: {}'.format(prepared.shape))\n",
    "dataset = pd.get_dummies(prepared)\n",
    "print('Dataset shape AFTER onehot encoding: {}'.format(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "This function will be called to run a linear regression model over the dataset passed as argument. It splits it into train and test, trains and evaluate the test set to finally return the R2 metric as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def score_model(data):\n",
    "    X = data.loc[:, data.columns != 'SalePrice']\n",
    "    y = data.loc[:, 'SalePrice']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    # print('Coefficients: \\n', regr.coef_)\n",
    "    \n",
    "    # The metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Pipeline\n",
    "\n",
    "### SumSF, sumBaths, sumPorch\n",
    "\n",
    "Simply sum the square feet of the different parts of the house into a single column called HouseSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_SF(df):\n",
    "    columns_to_add = ['1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2']\n",
    "    df['House_SF'] = df[columns_to_add].fillna('').sum(axis=1)\n",
    "    df.drop(columns_to_add, axis=1)\n",
    "    return df\n",
    "\n",
    "def sum_Baths(df):\n",
    "    df['Total_Baths'] = (df['FullBath'] + \n",
    "                         df['BsmtFullBath'] + \n",
    "                         (0.5*df['HalfBath']) + \n",
    "                         (0.5*df['BsmtHalfBath']))\n",
    "    df.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1)\n",
    "    return df\n",
    "\n",
    "def sum_Porch(df):\n",
    "    columns_to_add = ['OpenPorchSF','3SsnPorch','EnclosedPorch','ScreenPorch','WoodDeckSF']\n",
    "    df['Porch_sf'] = df[columns_to_add].sum(axis=1)\n",
    "    df.drop(columns_to_add, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "We use the statsmodels package to solve this issue. Let's do a little bit more in-depth and rigorous analysis first on outliers. I'll employ Leave-One-Out methodology with OLS to find which points have a significant effect on our model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renero/Code/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def remove_outliers(df):\n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    ols = sm.OLS(endog = y, exog = X)\n",
    "    fit = ols.fit()\n",
    "    test = fit.outlier_test()['bonf(p)']\n",
    "    outliers = list(test[test<1e-3].index) \n",
    "    df.drop(df.index[outliers])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-representation\n",
    "\n",
    "Eliminate those columns with most of the information belonging to the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_represented_features(df):\n",
    "    under_rep = []\n",
    "    for i in df.columns:\n",
    "        counts = df[i].value_counts()\n",
    "        zeros = counts.iloc[0]\n",
    "        if ((zeros / len(df)) * 100) > 99.0:\n",
    "            under_rep.append(i)\n",
    "    df.drop(under_rep, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation evaluation\n",
    "\n",
    "Cross validation implies that instead of evaluating the model with a single split of train/test, we will use a k-fold technique, and that will produce different results. A realistic outcome would be the mean of all that process, but if you can check how far your model can reach by simply changing the training/test datasets using CV, I decided to take the 'max()' in the last line of the function. Try with the mean() to check some other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def cv_evaluate(df):\n",
    "    lm = LinearRegression()\n",
    "    kfolds = KFold(n_splits=10, shuffle=True, random_state=23)\n",
    "\n",
    "    def cv_r2(model):\n",
    "        r2 = cross_val_score(model, X, y, scoring='r2', cv=kfolds)\n",
    "        return r2\n",
    "    \n",
    "    X = df.drop(['SalePrice'], axis=1)\n",
    "    y = df.SalePrice.reset_index(drop=True)\n",
    "    benchmark_model = make_pipeline(RobustScaler(), lm).fit(X=X, y=y)\n",
    "    return cv_r2(benchmark_model).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score: 0.8766\n",
      "- New Score (sum_SF): 0.8766 [diff: 0.0000] [Accepted]\n",
      "- New Score (sum_Baths): 0.8766 [diff: -0.0000] [Accepted]\n",
      "- New Score (sum_Porch): 0.8766 [diff: 0.0000] [Accepted]\n",
      "- New Score (under_represented_features): 0.8771 [diff: 0.0006] [Accepted]\n",
      "Max R2 after CV: 0.9344\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "# I'm not adding the 'remove_outliers' to save time, since that step takes some minutes to complete.\n",
    "# Your should add it to check the results for yourself\n",
    "fe_functions = ['sum_SF','sum_Baths','sum_Porch','under_represented_features']\n",
    "funcs, new_dataset = feature_engineering_pipeline(dataset, fe_functions)\n",
    "\n",
    "# After running the FE pipeline, I check how CV make my results shuffle, as it takes \n",
    "# different portions of the data each time, and that causes different metric values\n",
    "r2 = cv_evaluate(new_dataset)\n",
    "print('Max R2 after CV: {:.4f}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
