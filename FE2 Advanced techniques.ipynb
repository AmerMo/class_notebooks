{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced techniques\n",
    "\n",
    "Let's explore feature engineering techniques with the house prices dataset from Kaggle.\n",
    "\n",
    "We can find an illustrative example of how to use Deep feature synthesis [here](https://www.kaggle.com/willkoehrsen/featuretools-for-good), and a good explanation [here](https://stackoverflow.com/questions/52418152/featuretools-can-it-be-applied-on-a-single-table-to-generate-features-even-when).\n",
    "\n",
    "To Do\n",
    "- Group some features together (business domain)\n",
    "- Find categories under-represented (beyond current proposed method)\n",
    "- ~~Scale numerical features~~\n",
    "- Onehot/Label binarize encode categorical features (option)\n",
    "- Baseline the model with a multiple linear regression and high degree polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Advanced-techniques\" data-toc-modified-id=\"Advanced-techniques-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Advanced techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup-the-dataset\" data-toc-modified-id=\"Setup-the-dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup the dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Creation\" data-toc-modified-id=\"Feature-Creation-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Feature Creation</a></span></li><li><span><a href=\"#Scale-numerical-features\" data-toc-modified-id=\"Scale-numerical-features-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Scale numerical features</a></span></li><li><span><a href=\"#Check-skewness\" data-toc-modified-id=\"Check-skewness-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Check skewness</a></span></li><li><span><a href=\"#Check-correlation\" data-toc-modified-id=\"Check-correlation-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Check correlation</a></span></li><li><span><a href=\"#Under-represented-features\" data-toc-modified-id=\"Under-represented-features-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Under represented features</a></span></li><li><span><a href=\"#OneHot-encoding-for-categorical-variables.\" data-toc-modified-id=\"OneHot-encoding-for-categorical-variables.-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>OneHot encoding for categorical variables.</a></span></li><li><span><a href=\"#Baseline-basic-all-numeric-features\" data-toc-modified-id=\"Baseline-basic-all-numeric-features-1.1.7\"><span class=\"toc-item-num\">1.1.7&nbsp;&nbsp;</span>Baseline basic all-numeric features</a></span></li></ul></li><li><span><a href=\"#Deep-Feature-Synthesis\" data-toc-modified-id=\"Deep-Feature-Synthesis-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Deep Feature Synthesis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-EntitySet\" data-toc-modified-id=\"Build-the-EntitySet-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Build the EntitySet</a></span></li><li><span><a href=\"#Normalize-the-entity\" data-toc-modified-id=\"Normalize-the-entity-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Normalize the entity</a></span></li><li><span><a href=\"#Deep-feature-synthesis\" data-toc-modified-id=\"Deep-feature-synthesis-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Deep feature synthesis</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import featuretools as ft\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, \\\n",
    "        validation_curve, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from src.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('int64') dtype('O') dtype('float64')]\n",
      "80 Features\n",
      "43 categorical features\n",
      "37 numerical features\n",
      "16 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "64 Complete features\n",
      "--\n",
      "Target: Not set\n"
     ]
    }
   ],
   "source": [
    "houses = Dataset('./data/houseprices_prepared.csv.gz')\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace the NA's in the dataset with 'None' or 'Unknown' since they're not really NA's. For no good reason the person in charge of encoding the file decided to assign NA's to values where the feature does not apply, but instead of using a value for that special condition (like the string 'None') he/she decided to use the actual NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('int64') dtype('O') dtype('float64')]\n",
      "79 Features\n",
      "43 categorical features\n",
      "36 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "79 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "houses.replace_na(column='Electrical', value='Unknown')\n",
    "houses.replace_na(column=houses.names('categorical_na'), value='None')\n",
    "houses.set_target('SalePrice')\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "\n",
    "This is the part where we decide to remove or add features based on our knowledge on the data and phenomena being represented. In this case, we're removing the `Id` field, and summing up some of the numerical fields counting the nr. of bathrooms, porchs and square feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('int64') dtype('O') dtype('float64')]\n",
      "68 Features\n",
      "43 categorical features\n",
      "25 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "68 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "def sum_SF():\n",
    "    columns_to_add = ['1stFlrSF','2ndFlrSF','BsmtFinSF1','BsmtFinSF2']\n",
    "    houses.features['House_SF'] = houses.\\\n",
    "        features[columns_to_add].fillna('').sum(axis=1)\n",
    "    houses.drop_columns(columns_to_add)\n",
    "\n",
    "def sum_Porch():\n",
    "    columns_to_add = ['OpenPorchSF','3SsnPorch','EnclosedPorch',\n",
    "                      'ScreenPorch','WoodDeckSF']\n",
    "    houses.features['Porch_sf'] = houses.features[columns_to_add].sum(axis=1)\n",
    "    houses.drop_columns(columns_to_add)\n",
    "\n",
    "def sum_Baths():\n",
    "    columns_to_add = ['FullBath', 'BsmtFullBath', 'HalfBath', 'BsmtHalfBath']\n",
    "    houses.features['Total_Baths'] = (\n",
    "        houses.features ['FullBath'] + \n",
    "        houses.features ['BsmtFullBath'] + \n",
    "        (0.5*houses.features['HalfBath']) + \n",
    "        (0.5*houses.features['BsmtHalfBath']))\n",
    "    houses.drop_columns(columns_to_add)\n",
    "\n",
    "houses.drop_columns('Id')\n",
    "sum_SF()\n",
    "sum_Porch()\n",
    "sum_Baths()\n",
    "houses.metainfo()\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical features\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('O')]\n",
      "68 Features\n",
      "43 categorical features\n",
      "25 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "68 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "houses.scale()\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check skewness\n",
    "\n",
    "In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('O')]\n",
      "68 Features\n",
      "43 categorical features\n",
      "25 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "68 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "houses.ensure_normality()\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 correlated columns to remove.\n",
      "['TotRmsAbvGrd', 'GarageArea', 'House_SF', 'Total_Baths']\n"
     ]
    }
   ],
   "source": [
    "numericals_to_drop, corr_num = houses.numerical_correlated(threshold=0.7)\n",
    "print('There are {} correlated columns to remove.'.format(\n",
    "    len(numericals_to_drop)))\n",
    "print(numericals_to_drop)\n",
    "# houses.plot_corr_matrix(corr_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 correlated columns to remove.\n",
      "['Exterior2nd', 'GarageCond']\n"
     ]
    }
   ],
   "source": [
    "categoricals_to_drop, corr_categ = houses.categorical_correlated(threshold=0.7)\n",
    "print('There are {} correlated columns to remove.'.format(\n",
    "    len(categoricals_to_drop)))\n",
    "print(categoricals_to_drop)\n",
    "# houses.plot_corr_matrix(corr_categ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('O')]\n",
      "62 Features\n",
      "41 categorical features\n",
      "21 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "62 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "houses.drop_columns(categoricals_to_drop + numericals_to_drop)\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under represented features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with unrepresented categories:\n",
      " ['Street', 'Utilities', 'Condition2', 'RoofMatl', 'PoolQC']\n",
      "\n",
      "Available types: [dtype('float64') dtype('O')]\n",
      "57 Features\n",
      "36 categorical features\n",
      "21 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "57 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "urf = houses.under_represented_features()\n",
    "print('Features with unrepresented categories:\\n', urf)\n",
    "houses.drop_columns(urf)\n",
    "print(end='')\n",
    "houses.describe();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHot encoding for categorical variables.\n",
    "\n",
    "Convert categorical variable into dummy/indicator variables. I use pandas `get_dummies` for this task. Beware of not using this before measuring correlation, as it will destroy your measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('uint8')]\n",
      "242 Features\n",
      "0 categorical features\n",
      "242 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "242 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "houses_prepared = copy(houses)\n",
    "houses.onehot_encode()\n",
    "houses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline basic all-numeric features\n",
    "\n",
    "Time to assess what can a simple and multiple linear regression can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = houses.split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "cv = ShuffleSplit(n_splits=1000, test_size=0.2, random_state=666)\n",
    "scores = cross_val_score(model, \n",
    "                         X.train, y.train, \n",
    "                         cv=cv, \n",
    "                         scoring='r2')\n",
    "print('Obtained {} positive R2 scores'.format(len(scores[scores > 0.0])))\n",
    "print('Best Validation R2: {:.2f}'.format(max(scores)))\n",
    "print('Avg. Validation R2: {:.2f}'.format(np.mean(scores[scores > 0.0])))\n",
    "\n",
    "sns.distplot(scores[scores > 0.0], hist = False, kde = True, \n",
    "             kde_kws = {'shade': True, 'linewidth': 3});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we can evaluate our regression problem using CV, but with a 1'st degree polynomial which is clearly too simple for this problem. The symptom of this is the extremely negative values of the R2 scores obtained.\n",
    "\n",
    "To improve our solution, let's build a linear regression model with a higher degree polynomial. To do so, a scikit learn pipeline is used, where PolynomialFeatures is used before the linear regression, to try out different `degree` polynomials.\n",
    "\n",
    "`PolynomialFeatures` generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=123)\n",
    "pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False), \n",
    "    LinearRegression(n_jobs=-1)).fit(X.train, y.train)\n",
    "scores = cross_val_score(pipeline, X.train, y.train,\n",
    "                         scoring=\"r2\", cv=cv)\n",
    "\n",
    "print('Obtained {} positive R2 scores'.format(len(scores[scores > 0.0])))\n",
    "print('Best CV R2: {:.2f}'.format(max(scores)))\n",
    "print('Avg. CV R2: {:.2f} +/- {:.02}'.format(\n",
    "    np.mean(scores[scores > 0.0]),\n",
    "    np.std(scores[scores > 0.0])))\n",
    "print('R2 in hold-out dataset: {:.2f}'.format(\n",
    "    pipeline.score(X.test, y.test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information obtained in the cross validation process, I know that I'm not overfitting, so my results seem to be OK. The $R^2$ obtained is decent. But I don't know which of my splits is producing the best possible result. So, at this point, I can rely on a single fit, or I can try to use the model trained with the split that produces the best generalization error.\n",
    "\n",
    "To do so:\n",
    "\n",
    "  1. I use `cross_validate` method instead of `cross_val_score`, and I also specify that I want the estimator trained with each split to be returned\n",
    "  2. I score all the different estimators to see which one is producing the best generalization error over the hold-out dataset (`X.train` and `y.train`).\n",
    "  3. Compare the results obtained with a single estimator over the entire dataset, with the results obtained over a single split (the one producing the best results in generalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('polynomials', PolynomialFeatures(degree=2, include_bias=False)), \n",
    "    ('linear_regression', LinearRegression(n_jobs=-1))])\n",
    "pipeline.fit(X.train, y.train)\n",
    "training_score = pipeline.score(X.test, y.test)\n",
    "print('R2 from entire-dataset estimator: {:.2f}'.format(training_score))\n",
    "\n",
    "# Obtain scores and estimators from different splits and use the best one.\n",
    "scores = cross_validate(pipeline, \n",
    "                        X.train, y.train,\n",
    "                        scoring=['r2'], \n",
    "                        cv=5,\n",
    "                        return_estimator=True)\n",
    "split_scores = [scores['estimator'][i].score(X.test, y.test) \n",
    "                for i in range(len(scores))]\n",
    "index_best = split_scores.index(max(split_scores))\n",
    "print('Best estimator R2 score: {:.2f}'.format(split_scores[index_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Synthesis\n",
    "### Build the EntitySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-28 22:05:35,102 featuretools.entityset - WARNING    index Id not found in dataframe, creating new integer column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Entityset: None\n",
       "  Entities:\n",
       "    houses [Rows: 1460, Columns: 59]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet()\n",
    "es = es.entity_from_dataframe(entity_id='houses', \n",
    "                              dataframe=pd.concat([houses_prepared.features, \n",
    "                                                   houses_prepared.target], \n",
    "                                                  axis=1),\n",
    "                              index = 'Id')\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: None\n",
       "  Entities:\n",
       "    houses [Rows: 1460, Columns: 59]\n",
       "    houses_norm [Rows: 1460, Columns: 1]\n",
       "  Relationships:\n",
       "    houses.Id -> houses_norm.Id"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables = houses_prepared.names('all') + [houses_prepared.target.name]\n",
    "es.normalize_entity(base_entity_id='houses', \n",
    "                    new_entity_id='houses_norm',\n",
    "                    index='Id')\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep feature synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 204 features\n",
      "EntitySet scattered to workers in 3.383 seconds\n",
      "Elapsed: 00:09 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object add_client at 0x119b83fc0>\n",
      "RuntimeError: generator ignored GeneratorExit\n",
      "tornado.application - ERROR - Exception in Future <Future cancelled> after timeout\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/renero/Code/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 970, in error_callback\n",
      "    future.result()\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_matrix, f_defs = ft.dfs(entityset=es,\n",
    "                          target_entity='houses_norm', \n",
    "                          verbose=1,\n",
    "                          n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove new variables that might be related to the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to drop columns: ['SUM(houses.SalePrice)', 'STD(houses.SalePrice)', 'MAX(houses.SalePrice)', 'SKEW(houses.SalePrice)', 'MIN(houses.SalePrice)', 'MEAN(houses.SalePrice)']\n"
     ]
    }
   ],
   "source": [
    "drop_cols = []\n",
    "for col in f_matrix:\n",
    "    if col == houses_prepared.target.name:\n",
    "        pass\n",
    "    else:\n",
    "        if houses_prepared.target.name in col:\n",
    "            drop_cols.append(col)\n",
    "            \n",
    "print('Need to drop columns:', drop_cols)\n",
    "f_matrix = f_matrix[[x for x in f_matrix if x not in drop_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:61640 remote=tcp://127.0.0.1:61609>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to remove 63 columns with >= 0.99 correlation.\n"
     ]
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = f_matrix.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\n",
    "\n",
    "print('Need to remove {} columns with >= 0.99 correlation.'.format(len(to_drop)))\n",
    "f_matrix = f_matrix[[x for x in f_matrix if x not in to_drop]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('int64') dtype('O')]\n",
      "193 Features\n",
      "72 categorical features\n",
      "121 numerical features\n",
      "0 categorical features with NAs\n",
      "42 numerical features with NAs\n",
      "151 Complete features\n",
      "--\n",
      "Target: Not set\n"
     ]
    }
   ],
   "source": [
    "fs_df = pd.concat(\n",
    "    [f_matrix, houses_prepared.features, houses_prepared.target], \n",
    "    axis=1)\n",
    "fs = Dataset.from_dataframe(fs_df)\n",
    "fs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP local=tcp://127.0.0.1:61623 remote=tcp://127.0.0.1:61609>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available types: [dtype('float64') dtype('int64') dtype('uint8')]\n",
      "562 Features\n",
      "0 categorical features\n",
      "562 numerical features\n",
      "0 categorical features with NAs\n",
      "0 numerical features with NAs\n",
      "562 Complete features\n",
      "--\n",
      "Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "fs.replace_na(column=fs.names('numerical_na'), value=0)\n",
    "fs.set_target(houses_prepared.target.name)\n",
    "fs.onehot_encode()\n",
    "fs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fs.split()\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=123)\n",
    "pipeline = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False), \n",
    "    LinearRegression(n_jobs=-1)).fit(X.train, y.train)\n",
    "scores = cross_val_score(pipeline, X.train, y.train,\n",
    "                         scoring=\"r2\", cv=cv)\n",
    "\n",
    "print('Obtained {} positive R2 scores'.format(len(scores[scores > 0.0])))\n",
    "print('Best CV R2: {:.2f}'.format(max(scores)))\n",
    "print('Avg. CV R2: {:.2f} +/- {:.02}'.format(\n",
    "    np.mean(scores[scores > 0.0]),\n",
    "    np.std(scores[scores > 0.0])))\n",
    "print('R2 in hold-out dataset: {:.2f}'.format(\n",
    "    pipeline.score(X.test, y.test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
